date,language,stargazers,starsToday,description,owner,name,url
2023-07-22,python,31969,1352,Inference code for LLaMA models,facebookresearch,llama,https://github.com/facebookresearch/llama
2023-07-22,python,5585,2373,Advanced Python Mastery (course by @dabeaz),dabeaz-course,python-mastery,https://github.com/dabeaz-course/python-mastery
2023-07-22,python,1215,431,Examples and recipes for Llama 2 model,facebookresearch,llama-recipes,https://github.com/facebookresearch/llama-recipes
2023-07-22,python,264,45,,SamsungLabs,NeuralHaircut,https://github.com/SamsungLabs/NeuralHaircut
2023-07-22,python,14320,90,"Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",microsoft,unilm,https://github.com/microsoft/unilm
2023-07-22,python,1524,305,Open source implementation of the ChatGPT Code Interpreter,shroominic,codeinterpreter-api,https://github.com/shroominic/codeinterpreter-api
2023-07-22,python,236,23,"low-cost, high-efficiency, easy-to-implement",Melelery,c-binance-future-quant,https://github.com/Melelery/c-binance-future-quant
2023-07-22,python,2073,31,大麦网抢票脚本,MakiNaruto,Automatic_ticket_purchase,https://github.com/MakiNaruto/Automatic_ticket_purchase
2023-07-22,python,307,64,Repo for adapting Meta LlaMA2 in Chinese! META最新发布的LlaMA2的汉化版！ （完全开源可商用）,michael-wzhu,Chinese-LlaMA2,https://github.com/michael-wzhu/Chinese-LlaMA2
2023-07-22,python,684,189,LLaMA v2 Chatbot,a16z-infra,llama2-chatbot,https://github.com/a16z-infra/llama2-chatbot
2023-07-22,python,18436,252,"A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.",oobabooga,text-generation-webui,https://github.com/oobabooga/text-generation-webui
2023-07-22,python,1334,269,"ShortGPT - An experimental AI framework for automated short/video content creation. Enables creators to rapidly produce, manage, and deliver content using AI and automation.",RayVentura,ShortGPT,https://github.com/RayVentura/ShortGPT
2023-07-22,python,8899,84,Practical Python Programming (course by @dabeaz),dabeaz-course,practical-python,https://github.com/dabeaz-course/practical-python
2023-07-22,python,2100,41,"Hey there new grad! We've put together a collection of full-time job openings for SWE, Quant, PM and tech roles in 2024!",ReaVNaiL,New-Grad-2024,https://github.com/ReaVNaiL/New-Grad-2024
2023-07-22,python,30721,133,Diagram as Code for prototyping cloud system architectures,mingrammer,diagrams,https://github.com/mingrammer/diagrams
2023-07-22,python,4901,8,"Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.",Lightning-AI,lit-llama,https://github.com/Lightning-AI/lit-llama
2023-07-22,python,12999,90,中文LLaMA&Alpaca大语言模型+本地CPU/GPU训练部署 (Chinese LLaMA & Alpaca LLMs),ymcui,Chinese-LLaMA-Alpaca,https://github.com/ymcui/Chinese-LLaMA-Alpaca
2023-07-22,python,108412,183,"Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",huggingface,transformers,https://github.com/huggingface/transformers
2023-07-22,python,175,16,LLaMA Cog template,a16z-infra,cog-llama-template,https://github.com/a16z-infra/cog-llama-template
2023-07-22,python,55658,176,Building applications with LLMs through composability,hwchase17,langchain,https://github.com/hwchase17/langchain
2023-07-22,python,1414,3,,SergeyPirogov,webdriver_manager,https://github.com/SergeyPirogov/webdriver_manager
2023-07-22,python,1131,40,Firefly(流萤): 中文对话式大语言模型(全量微调+QLoRA)，支持微调Llma2、Llama、Baichuan、InternLM、Ziya、Bloom等大模型,yangjianxin1,Firefly,https://github.com/yangjianxin1/Firefly
2023-07-22,python,25274,59,"An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.",lm-sys,FastChat,https://github.com/lm-sys/FastChat
2023-07-22,python,131,30,Uses Various AI Service APIs to generate memes with text and images,ThioJoe,Full-Stack-AI-Meme-Generator,https://github.com/ThioJoe/Full-Stack-AI-Meme-Generator
2023-07-22,python,6018,180,"Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading",bigscience-workshop,petals,https://github.com/bigscience-workshop/petals